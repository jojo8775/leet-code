
TinyUrl 

1. requrement 

- why do we need it. 
- given a long url it should return a short url 
- when the short url is visited it should redirect to the long url 
- user should optionally be able to pick a custom url 
- Url should have an expiry time. 
- non functional 
    - this should be highly available --- this means the service should not be down 
    - this should be reliable --- this mean there should not be any collition and data corruption 
    - the url should not be sequential --- this mean they should not be able to guess otherwise they can be abused 


2. Capacity estimate ---- this is also termed as the back of the hand calculation  :: this part I need to re-consider 
    - is this service a read heavy or write heavy 
    - consider the following calculations 
        -- its a read heavy service 
        -- 100 : 1 --- which means there will be a 100 read for each write 
        - 

        this mean there will be two kind of calculations 
            - traffic estimate :: defies the max number of concurrent request and 
                - 500 M new urls per month  --- can be considered as writes 
                - 100 * 500 M url redirections --- can be considered as reads 
                -- writes per seconds === 500 M / (30 * 24 * 3600) ~ 200 urls/s :: there is an easy way to calculate this. (Prime url)
                -- reads per seconds == 100 : 1 === 20 K urls/s

            - storage 
                - assume we store on avg all the urls for 5 years 
                --- 500 M * 12 * 5 == > 30 B 
                and assume each url is of 500 bytes 
                    --- 30 B * 500 bytes === 15 TB 

            - bandwidth estimate :: 
                - since the asuumption is per url it is 500 bytes 
                - for read  - 20 K * 500 b ~ 10 Mb/s 
                - for write - 200 * 500 ~ 100 kb/s 

            - cache --- this is 80 to 20 and LRU cache 
                 --- since there will be 200 k reads per second 
                     --- per day it will be 20 K * 24 * 36000 ~ 1.7 B 
                     --- 20%  memory for 1.7 B will be .2 * 1.7B * 500 Bytes = 170 GB


3. System API design 
    - for creating URL :: 
        createUrl(userId, originalUrl, customAlias, expireTime)
            userId - to throttle malicious users and prevent abuse 
            originalUrl - this is the orifinal url 
            customAlias - if consumer wants to provide a custim short url 
            expireTime - custom expire time, by default it will be 1 year 

        return : string :: sorted url 

    - for read URL : 
        no api is needed as there will be redirection based on the data persisted 

    - internal use or backend job 
        deleteUrl(shortenedUrl, userId)

        userId - to prevent abuse 
        shoetenedUrl - the tiny url to be deleted 


4. Database design :: since the persistence model is very simple and there is not much relation, NoSql db will be used. 

    reason for NoSql 
        - read heavy service 
        - will be storing billions of records 
        - each record will be less than 1 Kb 
        - there is no complex relationship between models 

    there will be two tables 

    URL : 
        - original URL
        - createdDateTime --- this will be used to purge stored data 
        - expirationTime 
        - userId 

    User : 
        - userId 
        - userName 
        - userEmail 
        - createdDateTime 
        - lastLogin 






2. PasteBin 

- design considerations are very similar to the tiny url. 

    Design considerations 
        - people should be able to share a text and get a link 
            - highly reliable --- there should not be any data loss 
            - highly available --- user should always be able to create a new entry
            - there should be a high limit of 10 MB to prevent abuse 
            - there should be ability to create custom url --- but there is a limitation on the length of the url  
        - this will be a read heavy service but now as busy as twitter and facebook 


- Capacity estimates :: 

    The goal is to measure 
            - traffic estimate --- Start of with an estimate for a month. The read and write should be based on the ratio finalized in the design consideration. 
            - storage estimate --- this should be estimated for around 5 years 
            - bandwidth estimate --- this is where req and res count per second will be needed. To reach this place there should be an idea of how big is each entry 
            - Cache consideration --- thumb rule is 80 - 20 but then LRU or LFU that needs to be evaluated. 


    Traffic estimate :: 
        Lets consider 5:1 read to write ratio 
        - let say there are 1 M writes per day 
        - 5M reads per day 
        - new paste per second = 1M / (24 * 3600) ~ 12 paste per seconds 
        - reads per second --- 12 * 5 = 60 

    Storage estimates ::
        most of the time people will be storing logs and snippets in the pastebin which are not that big. So assume 10kb per paste. 

        - write will need 1 M * 10kb = 10GB/day 
        - for 10 years (we can also keep it a standard of 5 year)
            10 GB * 365 (days) * 10 (years) ~ 36 TB 


        -- now calculating the number of unique urls which will be needed

         365M  * 10 ~ 3.6 Billion urls 
            --- 64 base encoding will be use ... but how many characters will be enough? 
                6 character ===  64 ^ 6 ~ 68.7 Billion :: this quantifies that 6 characters will be sufficient 

        -- now calculate how much storage will be needed for these URLs 
            - if each char takes 1 byte then each url will take 6 bytes 

            6b * 3.6 Billion ~ 22 GB 


        36 TB + 22 GB ~ 36 TB 

        We would aim for 70% as the peak capacity as buffer so we will need 51.4 TB 

    Bandwidth estimates : 

        - writes per second = 1 Million / (24 * 3600) ~ 12 pastes per second 
        - 10kb * 12 = 120 kb/s 

        - reads per second = 12 * 5 ~ 60 reads per second 
        - 10kb * 60 = 600 kb/s 

        -- totals  = 120 + 600 ~ 720 kbps 

    Cache consideration :: 80 - 20 ratio --- so 

- API 
    - API to insert 
        string paste(user_id, content, custom_url, expiration_date)

        user_id - to protect from issues like abuse 
        content - the content in the paste 
        custom_url - if consumer choose to provide one 
        expiration_date - the expiration date of the document 

        return string - the tiny url of the paste if successfully created 

    - API to get

        string get(tiny_url)

        tiny_url - this is the url of the paste 


        returns string - the content of the paste 

    - API to delete 
        bool delete(tiny_url)

        tiny_url - the url to delete

        returns bool - this is true when deletion is successful and false if there is no url preset to delete

- basic design 

    - on getting a new request for the paste the application server should assign a new unique key
    - each application server should get a set of unique key from the key server
        - the key generator server should generate all the unique keys and maintain two tables. One table for new keys and other for used keys 
        - the key generator should cache some keys for faster response 
        - the cached keys should be marked as used keys 
        - if the key generator dies then a backup key generator should pick up 
        - the unused cached keys in the key generator will be lost 

    - each application server
        - should preserve a set of keys so that a new one can be quickly provided 
        - if an application server dies then the unused keys will be lost 









 TCP LB vs HTTP LB
 



Messaging Queue 
----------------



User --> Gateway api ---> Front End Server  ---> Meta data service              --- > connects to the database 
        1 - *       1 - *                       1. stores the information of the queue 
                     --->                       2. stores cache 
                                                3. read heavy server 
                     1. Req vaidation           
                     2. SSL termination 
                     3. SS encription 
                     4. rate limiting 
                     5. cache 
                     6. real time valdiation 


Replication of the meta data server 
1. all server have the entire cache --- no partition is done to the cached data --- can be done only when the cache is small. 
    --- in this case we can add load balancer in front of it 
2. the cache is too big for one node to store --- shard it base on the message hash and front end data knows about the shard to call 
3. the same setup as above but this time front end server knows nothing about the destination shard and calls a random shard 
    --- the random shard calls the other server. 



front end server --> metadata service --> data base 


Back end services : 
    where and how the message will be stored 
    --- can I use DB for the message --- ? no because the through put is very high. 
    --- Read about the distributed database 
    --- other alternative? in memory or file system 


    : we will be using an in-memory storage or disk storage --- since high throughput is needed 

    Replication : 
        Leader - follower : in this case there has be an in-host manager to map queue with leader and followers instances within a cluster  
        Leader less model : each cluster will replicate data to other clusters --- we can use quorum read / write technique 
            --- this will need a queue with cluster manager 

    user --> Front end server --> 1. Metadata store to get the queue information 
    (send req)                --> 2. calls a random cluster for that queue from in-host / out-host manager 

    user --> Front end server --> 1. Metadata store to get the queue information 
    (receive req)                --> 2. calls a random cluster for that queue from in-host / out-host manager 


total diagram : 

user (sender) --> gateway --> load balancer -> front end server --> 1. metadata storage server ---> cache / data base 
                                                                --> 2. sends the data to message cluster using information from zoo keeper 

user (receiver) --> gateway --> load balancer -> front end server --> 1. metadata storage server ---> cache / data base 
                                                                --> 2. receives message from any cluster using information from zoo keeper 

Mechanism used will be long poll mechanism. --- > 







Final round of preparation: 

Thursday: do a self mock 

1. Web top K, system to count. [done]
    --- this covers trending, top viewed, viewed video count, trading info. 
2. Youtube, Netflix or any streaming service. 
3. Web crawler
4. Facebook, Instagram, Twitter 
5. Twitter search, type safety. 


Friday 
1. Uber, Yelp
2. distributed cache [done]
3. distributed queue. --- Messaging queue [done]
4. Tiny Url, paste bin
5. rate limiter [done]

Saturday 
1. amazon leadership principles 
2. some recent coding problems --- may be 5 

Sunday 
1. amazon leadership principles 
2. some recent coding problems --- around 6-7

Monday 
1. one final leftover sweep of system design 
2. one final sweep on the scalable data store 

Tuesday 
-- just relax 

Wednesday 
1. give interview 
2. relax after that, celebrate. Focus at work from next day. 




https://www.youtube.com/watch?v=DphnpWVYeG8&ab_channel=DataCouncil





Web Crawler 

Functional Requirement 
1. User should be able to submit a seed url with priority 
2. The crawler should crawl all the http urls for now, but should be extendable 
3. The crawler should be able to detect data freshness of the crawled document 

Non functional 
1. It should not use the same url more than once 
2. It should be polite to other server and not overload them 

Scale 

Daily storage of the documents 



Twitter 

Functional Requirement 
1. user can post tweets 
2. user can follow others 
3. user will have news feeds 

Non functional 
1. the latency should be low and the system should scale 
2. once a tweet is posted it should not be lost 
3. the system should be available, there should be no down time. 



Estimates : 

Storage : 
10 M users on average tweets 10 per day 

100 M tweets per day 

Each tweets : 140 character : 280 bytes 
it will metadata 
    User id 
    timestamp 

    total metadata : 50 bytes 

    320 * 100,000,000 = 32 GB / day 


Bandwidth : 
    100M active users 

    10 requests per day each

    1 Billion requests per day total 


API 


Post(userId, tweetContent, timestamp)

follow(followerId, followeeId)

feed(userId)

search(userId, searchTerm, numberOfRecords, pageToken)


High level design : 






Rate limiter : 

functional requirement : 

when a service calls, it should get a return value of true or false


Non functional requirement 

1. should have low latency
2. should be easy to scale 
3. should not throttle service if not needed. 




public class RateLimiter{
    
    private long capacity, maxCapacity, fillRatePerNanosecond;
    private long lastRefill;

    public RateLimiter(long fillRatePerSecond){
        fillRatePerNanosecond = fillRatePerSecond * 10e9;
        maxCapacity = fillRatePerSecond;
        lastRefill = System.Time.nanosecond();
    }

    public boolean isAllowed(long token){
        refill();


        if(capacity - token >= 0){
            true;
        }

        return false;
    }

    private voice refill(){
        long now = System.Time.nanosecond();

        long diff = (now - lastRefill) * fillRatePerSecond;

        capacity = Math.min(maxCapacity, capacity + diff);
    }

}


Top K Rate limiter:

functional requirements: 
1. It should show the top k list in real time, i.e min 1 min lag 
2. Its should also be able to show top K list for longer duration such as few hours. 

Non functional requirements: 
1. The system should be able to handle many request (Scalable)
2. Accuracy of the service may suffer. 
3. This service should not effect the performance of the actual service. In other words, the resource used to count top k events, it should not take resource allocation from the real service. 


Initial design. 


Twitter: 


Functional Requirement 
1. User should be able to post Tweets 
2. User should be able to follow others Tweet in their homepage 
3. User should be able to follow what is trending 
4. User should be able to perform search for tweets 

Non-Functional Requirement 
1. The system should be responsive - Low latency 
2. No data loss, once a tweet is posted is should be persisted - Reliability
3. Should be able to handle larger load - easy to Scale 
4. There should be no down time - Available. 


Capacity 

Active users posting per day = 10 Million 
number of tweets posted per day = 5 

Total posts per day 
5 * 10 M = 50 M 

Each Tweets are about 300 Bytes 
Data store for one day = 300 * 50,000,000 = 15,000,000,000 ~ 15 GB
--- need to store the data in the multiple partition for which No-Sql will be a better option

number of user browsing 50 M 
each user browses 5 page and each page 10 tweets 
 
number of tweets browsed per day = 50,000,000 * 50 = 2.5 billion 
--- this shows the system is read heavy and need to use cache for low latency 


API

Post (user id, tweet lat, tweet long, user lat , user long, tweet)
Search (User id, search string)
feeds(user id)
timeline (user id)


Storage tables 

tweet table 
    - tweet id  (key)
    - user id 
    - tweet loc 
    - user loc 
    - tweet content 
    - link to media 

User Table 
    - User id (key)
    - email 
    - first name 
    - last name 
    - address 

User_Tweet_table
    - user id (key)
    - tweet id 

user_follower_table 
    - user id 
    - followees 
    - followers


Initial Design


Distributed Cache 

Functional Requirements : 
1. be able to call the cache to get item 
2. be able to call the cache to put item

Non Functional Requirement 
1. Cache should be very quick : Low latency 
2. Should be able to address large traffic : Scalability 
3. Cache should survive node outage and network outage : availability  


Initial design 






Front end web services : 
    --- this is a light weight web services 

    Actions : 
        1. SSL termination 
        2. Authentication 
        3. Request validation 
        4. Server side encryption 
        5. logging 
        6. caching 
        7. rate limiter --- throttling 


Distributed Queue 

Functional Requirement : 
1. should be able to put message for a topic 
2. should be able to get message 
3. should be able to create a topic 
4. should be able to remove a topic 


Non functional requirement :
1. should be able to handle large number of request : Should be scalable 
2. should be resilient to partial network failure : should be available 
3. low response time : low latency 
4. the message should not be lost. : durable 


Initial design 


Design online file system 

Functional Requirement 
1. Should be able to create a file 
2. Should be able to share a file 
3. Should be able to delete a file 
4. Should be able to update an existing file 

Non functional requirement 
1. client should be able to sync file efficiently : efficient 
2. the file updates should be consistent among all the shared clients 
3. no data loss. 
4. user dont have to worry about the space. Scalability 


Initial design : 



Youtube 

Functional Requirement : 
1. should be able to upload video
2. should be able to stream video 
3. 





Deliver result 
- needed critical help 
- went out of way to help team 
- pitched in and understood what is the problem 
- drove the improvement 

Have backbone and disagree and commit 
- client satisfaction 
- educate so that dont end up paying more 
- 

Deep dive 
- drill down and understand a system 
# root cause for any ICM


Earn trust 
- 5 whys 
- analyse and give feed back with solution 
- learned from previous manager 
# expertise in the area 


frugality 
- on-call 
- share of knowledge 
- pitch in from different sections 
# Azure service scale down 

Bias of Action 
- outliers --- 
- technical debt got caused 
- patch fix 
# cutting corners 

Think big **
- work backward with an end in mind 
- visualize the feedback of the solution .... not limited to the short fall of the previous system 
# forward thinking : how the service will be after 4-5 years 

Insist on the highest standard 
- insist on e2e tests 
- stress testing 
- make sure the non-functional requirements are met 
- easy to loose customers based on the degradation of the customer experience 
- reputational damage to the company that I stand for and that is a very responsibility 
# code reviews and increase test coverage


Hire and develop the best 
- groomed interns ... 
# mentor and improved the strong points and reduced the negative points 

Learn and Be Curious 
- introduced something new and made it efficient 
- give real proper examples ... 
# learn and be curious 


Are right a lot 
- backgorund research and make decision based on that 
- validate decision from others learning to see others prespective 
- dont rely solely on how others have done 
- "Group Think" --- board room decision 
- confident pitch is not always the correct decision 


Invent and Simplify 
- white boarding 
- divide bigger problem to a smaller sections 
# reduced transient ICS by improving the exception logging and telemetry 

Ownership 
- report bugs if uncovered while testing. outside scope


Customer Obsession 
- Human centric design thinking approach to a problem 



## Exponential Moving Average (EMA) 